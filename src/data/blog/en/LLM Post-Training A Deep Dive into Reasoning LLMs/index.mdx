---
title: "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
description: >-
  This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs.
draft: false
authors:
  - main-author
pubDate: 2025-03-07
heroImage: ./heroImage.png
categories:
  - Paper Interpretation
  - Large Language Models
---
[LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/pdf/2502.21321)

这篇论文主要探讨了大型语言模型（LLM）的后训练（Post-Training）技术，特别是如何通过微调（Fine-Tuning）、强化学习（Reinforcement Learning, RL）以及推理时（Test-Time Scaling）扩展等方法来优化模型的推理能力、提升事实准确性，并更好地与用户意图和伦理要求对齐。

解释Test-Time Scaling：[OpenAI o1 技术初探1：整体框架，利用Test-Time Scaling Law提升逻辑推理能力](https://www.53ai.com/news/finetuning/2024101120345.html)

